{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuración e importe de paquetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql.types import FloatType, StringType, IntegerType, DateType\n",
    "from pyspark.sql.functions import udf, col, length, isnan, when, count\n",
    "import pyspark.sql.functions as f\n",
    "import os \n",
    "from datetime import datetime\n",
    "from pyspark.sql import types as t\n",
    "#from pandas_profiling import ProfileReport\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuración del controlador e inicio de sesion Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "path_jar_driver = '/home/jovyan/code/java_sqlServer/mssql-jdbc-12.4.2.jre8.jar'\n",
    "\n",
    "#Configuración de la sesión\n",
    "conf=SparkConf() \\\n",
    "    .set('spark.driver.extraClassPath', path_jar_driver) \\\n",
    "    .set('spark.driver.memory', '8g')  # Ajusta el tamaño de la memoria del driver según sea necesario\n",
    "\n",
    "\n",
    "spark_context = SparkContext(conf=conf)\n",
    "sql_context = SQLContext(spark_context)\n",
    "spark = sql_context.sparkSession\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conexión a fuente de datos y acceso a los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detalles de la conexión a la base de datos SQL de Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detalles de conexión a la base de datos SQL de Azure\n",
    "url = \"jdbc:sqlserver://sqlserver-calidadgobierno.database.windows.net:1433;database=sqldb-calidadgobierno\"\n",
    "usuario = \"sqladmin@sqlserver-calidadgobierno\"\n",
    "contraseña = \"y.HCuP4gJBb?)MS/6~W!+;\"\n",
    "driver = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función para convertir un Dataframe en una tabla para mi base de datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "def cargar_dataframe_a_tabla(dataframe: DataFrame, nombre_tabla: str):\n",
    "    \"\"\"\n",
    "    Método para cargar un DataFrame en una tabla de una base de datos.\n",
    "\n",
    "    Parámetros:\n",
    "    - dataframe: DataFrame que se cargará en la tabla.\n",
    "    - nombre_tabla: Nombre de la tabla en la base de datos.\n",
    "    \"\"\"\n",
    "    # Escribe el DataFrame en la base de datos como una tabla\n",
    "    dataframe.write.jdbc(url=url, table=nombre_tabla, mode=\"append\", properties={\"user\": usuario, \"password\": contraseña, \"driver\": driver})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funcion para leer una tabla de la base de datos y convertirla en dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def leer_tabla_a_dataframe(url, tabla, usuario, contraseña, driver):\n",
    "\n",
    "    # Configurar la conexión a la base de datos\n",
    "    properties = {\n",
    "        \"user\": usuario,\n",
    "        \"password\": contraseña,\n",
    "        \"driver\": driver\n",
    "    }\n",
    "\n",
    "    # Leer la tabla y convertirla en un DataFrame\n",
    "    df = spark.read.jdbc(url=url, table=tabla, properties=properties)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecturas tablas parametrizadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura Activo de información"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre de la tabla que se desea leer\n",
    "nombre_tabla = \"dbo.activo_informaciones\"\n",
    "\n",
    "# Llamar a la función para leer la tabla y obtener el DataFrame\n",
    "df_activo = leer_tabla_a_dataframe(url, nombre_tabla, usuario, contraseña, driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Asumiendo que spark es tu SparkSession y df_activo ya está definido\n",
    "def mostrar_activos(df):\n",
    "    # Mostrar solo las columnas id y nombre para que el usuario elija\n",
    "    activos = df.select(\"id\", \"nombre\")\n",
    "    activos.show(truncate=False)\n",
    "\n",
    "# Ejecutar la función para mostrar los activos\n",
    "#mostrar_activos(df_activo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+\n",
      "|id |nombre          |\n",
      "+---+----------------+\n",
      "|12 |clienteJuridico2|\n",
      "+---+----------------+\n",
      "\n",
      "+---+----------------+------------------+----------------+----------------+----------+-----------+------------+-------+--------------+-------------------+----------+--------------------------+--------------------------+\n",
      "|id |nombre          |tipo_clasificacion|confidencialidad|custodio        |integridad|dependencia|formato     |idioma |frecuencia_act|fecha_creacion     |empresa_id|created_at                |updated_at                |\n",
      "+---+----------------+------------------+----------------+----------------+----------+-----------+------------+-------+--------------+-------------------+----------+--------------------------+--------------------------+\n",
      "|12 |clienteJuridico2|Alta              |Alta            |Carlos Gutierrez|Alta      |Analitica  |SQL DATABASE|Español|Mensual       |2024-05-16 05:00:00|6         |2024-05-16 06:30:32.049177|2024-05-16 09:39:34.618614|\n",
      "+---+----------------+------------------+----------------+----------------+----------+-----------+------------+-------+--------------+-------------------+----------+--------------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def seleccionar_activo(df):\n",
    "    mostrar_activos(df)\n",
    "    activo_id = input(\"Por favor, introduce el ID del activo que deseas seleccionar: \")\n",
    "    try:\n",
    "        activo_id = int(activo_id)  # Convertir el ID a entero\n",
    "    except ValueError:\n",
    "        print(\"El ID del activo debe ser un número entero.\")\n",
    "        return None, None  # Retorna None si la conversión falla\n",
    "\n",
    "    activo_seleccionado = df.filter(df.id == activo_id)\n",
    "    if activo_seleccionado.count() == 0:\n",
    "        print(\"No se encontró un activo con ese ID.\")\n",
    "        return None, None\n",
    "    \n",
    "    activo_seleccionado.show(truncate=False)\n",
    "    nombre_activo = activo_seleccionado.select(\"nombre\").collect()[0][0]\n",
    "    return activo_id, nombre_activo\n",
    "\n",
    "# Ejecutar el proceso completo\n",
    "activo_id, nombre_activo = seleccionar_activo(df_activo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---------+--------------------+------+--------------------+----------+--------+--------------------+-----------+-----------+----------+-----------------+------------------------+------------------+-------------------+----------+---------------------+----------------------------+----------------------------+--------------------+---------------------+----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------------------------------------+------------------+-----------------+----+--------+---------------+-------+--------------------+--------------------+----------------+----------+--------------------+-------------------------------+-------------+---------------------+--------------------+----------------+--------------+----------+--------+---------+-------+---------+----------------------+--------------------+-------------+----------+--------------------+--------------------+-------------------+-------------+------------------+--------------------+-----------------+---------+------------------+----------+----------+------+------+----------+---------+\n",
      "|TIPCLI|TIPDOC|      NIT|              NOMBRE|TIPREP|           REP_LEGAL|NIT_REPRST| CIU_REP|         NOM_CIU_REP|COD_DEP_REP|NOM_PAI_REP| FECEXPREP|CIUDAD_EXPEDICION|NOMBRE_CIUDAD_EXPEDICION|NOM_DEP_EXPEDICIÓN|NOM_PAIS_EXPEDICIÓN|   TEL_REP|FECHA_DE_CONSTITUCION|CODIGO_LUGAR_DE_CONSTITUCION|NOMBRE_LUGAR_DE_CONSTITUCION|COD_DEP_CONSTITUCIÓN|NOM_PAIS_CONSTITUCIÓN|FEC_INGBCO|TOT_ACTIVOB|TOT_PASIVOB|TOT_PTRMNOB|TOT_VENTASB|TOT_GASTOSB|CAP_SOCIALB|OTR_INGRSOB|DETALLE_DE_OTROS_INGRESOS_NO_OPERACIONALES|INGRESOS_MENSUALES|EGRESOS_MENSUALES|CIIU|REGIONAL|CODIGO_REGIONAL|AGEHOMO|     CODIGO_OFICNA_D|        TIPO_EMPRESA|SECTOR_ECONOMICO|TEL_COMERC|  DIRECCION_COMERCIO|CODIGO_CIUDAD_DE_SEDE_PRINCIPAL|NUMERO_SOCIOS|IDENTIFICACION_SOCIO1|       NOMBRE_SOCIO1|TIPO_PRODUCTO_ME|ID_PRODUCTO_ME|ENTIDAD_ME|MONTO_ME|CIUDAD_ME|PAIS_ME|MONEDA_ME|REALIZA_OPERACIONES_ME|          TIP_VINCLC|CARGO_PUBLICO|CUAL_CARGO|FECINI_CARGO_PUBLICO|FECFIN_CARGO_PUBLICO|DESEMP_CARGO_ACTUAL|REP_LEGAL_ONG|TIENE_VINCULO_PEPS|   Dir_Representante|Posee_Cuentas_Ext|ESTGESCOM|VINCULACIONES_GURU| FECUAC_EF|    FECUAC|SCORER|Fuente|FechaCorte|id_activo|\n",
      "+------+------+---------+--------------------+------+--------------------+----------+--------+--------------------+-----------+-----------+----------+-----------------+------------------------+------------------+-------------------+----------+---------------------+----------------------------+----------------------------+--------------------+---------------------+----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------------------------------------+------------------+-----------------+----+--------+---------------+-------+--------------------+--------------------+----------------+----------+--------------------+-------------------------------+-------------+---------------------+--------------------+----------------+--------------+----------+--------+---------+-------+---------+----------------------+--------------------+-------------+----------+--------------------+--------------------+-------------------+-------------+------------------+--------------------+-----------------+---------+------------------+----------+----------+------+------+----------+---------+\n",
      "|     1|     3|900962442|CONSTRUCTORA PROG...|     1|CASTRO GONZALEZ D...|  86066178|50001000|VILLAVICENCIO (META)|         50|        COL|11/12/1998|         50001000|    VILLAVICENCIO (META)|                50|                COL|3108814088|            2/04/2016|                    50001000|        VILLAVICENCIO (META)|                  50|                  COL|20/04/2016|         32|          1|         50|         32|          1|         50|       NULL|                                      NULL|                 1|                1|4330|       5|         Bogota|   2901|2901 - Villavicencio|Sociedad Anonima ...|               9|3108814088|CL 15 5 64 ESTE P...|                       50001000|            1|             86066178|CASTRO GONZALEZ D...|               0|             0|      NULL|     0.0|     NULL|   NULL|     NULL|                     N|        Solo Cliente|            N|      NULL|                   0|                   0|               NULL|            N|                 N|         CR 19 18 63|                N| INACTIVO|                 0|20/04/2016|20/04/2016|  BAJO|Bodega|31/01/2024|        2|\n",
      "|     1|     3|860037381|COOPERATIVA EXTER...|     1|OJEDA DUQUE EDNA ...|  51868672|11001000|   BOGOTA D.C. (D.C)|         11|        COL|11/10/1985|         11001000|       BOGOTA D.C. (D.C)|                11|                COL|   7904702|           21/07/1970|                    11001000|           BOGOTA D.C. (D.C)|                  11|                  COL|29/11/2022|       1479|         63|       1417|        203|        183|       1335|          1|                                     OTROS|                17|               15|6492|       5|         Bogota|    512|     512 - Principal|ENTIDAD SIN ANIMO...|              34|   3419900|         CRA 1 12-41|                       11001000|            0|                 NULL|                NULL|               0|             0|      NULL|     0.0|     NULL|   NULL|     NULL|                     N|        Solo Cliente|            N|      NULL|                   0|                   0|                  N|            N|                 N|      CRA 21 159A 57|                N| INACTIVO|                 0|29/11/2022| 2/05/2023|  BAJO|Bodega|29/02/2024|        2|\n",
      "|     1|     3|901148829|AGENCIA PROMOTORA...|     1|MORENO TOVAR MIGU...|  71732398| 5045000|APARTADO (ANTIOQUIA)|          5|        COL|17/01/2018|          5001000|    MEDELLIN (ANTIOQUIA)|                 5|                COL|   8281783|           17/01/2018|                     5001000|        MEDELLIN (ANTIOQUIA)|                   5|                  COL|25/07/2023|       7977|       4336|       3641|       9275|       3863|         15|          1|                                  DIVERSOS|               772|              321|6621|       3|       Medellin|   3601|     3601 - Envigado|            LIMITADA|              17|3154482545|    CR 100 88 21 P10|                        5045000|            3|             71732398|MORENO TOVAR MIGU...|               0|             0|      NULL|     0.0|     NULL|   NULL|     NULL|                     N|        Solo Cliente|            N|      NULL|                   0|                   0|               NULL|            N|                 N|    CR 100 88 21 P10|                N|   ACTIVO|                 0|25/07/2023|18/12/2023|  BAJO|Bodega|31/12/2023|        2|\n",
      "|     1|     3|900228855|   OG MAQUITRANS SAS|     1|GALLEGO VARON OSC...|  93385275|73001000|     IBAGUE (TOLIMA)|         73|        COL| 3/07/1990|         73001000|         IBAGUE (TOLIMA)|                73|                COL|   2654725|           23/06/2008|                    73001000|             IBAGUE (TOLIMA)|                  73|                  COL|29/10/2021|      12793|       9731|       3062|      11928|       1449|       1000|         45|                           INDEMNIZACIONES|               994|              120|7730|       1|           Cali|   1301|       1301 - Ibague|            LIMITADA|              99|   2654725|CR 5 88 164 BRR E...|                       73001000|            1|             93385275|GALLEGO VARON OSC...|               0|             0|      NULL|     0.0|     NULL|   NULL|     NULL|                     N|        Solo Cliente|         NULL|      NULL|                   0|                   0|               NULL|         NULL|              NULL|ESTANCIA DEL VERG...|             NULL|   ACTIVO|                 0| 6/10/2022|26/01/2024|  BAJO|Bodega|31/01/2024|        2|\n",
      "|     1|     3|  1619328|CREATING BUILDING...|     1|NAVARRO MEDINA RI...|  79718863|    NULL|                NULL|         30|          0|26/07/2006|         11001000|       BOGOTA D.C. (D.C)|                11|                COL|      NULL|            2/06/2006|                    11001000|           BOGOTA D.C. (D.C)|                  11|                  COL| 4/08/2006|          6|       NULL|          0|          0|          0|          0|       NULL|                                      NULL|              NULL|             NULL|   0|       5|         Bogota|    505|505 - Centro Inte...|SOCIEDAD PRIVADA ...|               0|   2118824| CL 57 7-11 OF 14-05|                           NULL|            0|                 NULL|                NULL|               0|             0|      NULL|     0.0|     NULL|   NULL|     NULL|                     N|        Solo Cliente|         NULL|      NULL|                NULL|                NULL|               NULL|         NULL|              NULL|                NULL|             NULL| INACTIVO|                 0| 1/04/2013|25/04/2023|  BAJO|Bodega|31/12/2023|        2|\n",
      "|     1|     3|900962480|CONSTRUCTORA CASA...|     1|ZARATE CHAVARRIAG...|  79380150|11001000|   BOGOTA D.C. (D.C)|         11|        COL|16/08/1984|         11001000|       BOGOTA D.C. (D.C)|                11|                COL|   6124921|           20/04/2016|                    11001000|           BOGOTA D.C. (D.C)|                  11|                  COL|12/08/2016|       7704|       3792|       3912|       1026|        826|         90|        110|                               FINANCIEROS|               114|               91|4390|       5|         Bogota|    509|509 - Andino Avda 82|Sociedad Anonima ...|               9|   6202366|CRA 8 D 106 26 PI...|                       11001000|            2|             41693855|CLRA STELLA CASTILLO|               0|             0|      NULL|     0.0|     NULL|   NULL|     NULL|                     N|        Solo Cliente|            N|      NULL|                   0|                   0|               NULL|            N|                 N|     CRA 11 A 119 34|                N| INACTIVO|                 0|16/01/2019|16/01/2019|  BAJO|Bodega|31/01/2024|        2|\n",
      "|     1|     3|860037759|CONDIMENTOS LA MO...|     1|ALARCON PINILLA J...|  17136501|11001000|   BOGOTA D.C. (D.C)|         11|        COL|27/01/1967|         11001000|       BOGOTA D.C. (D.C)|                11|                COL|   7104224|            6/03/1973|                    11001000|           BOGOTA D.C. (D.C)|                  11|                  COL|14/07/2011|        372|        208|        164|        279|        238|         35|       NULL|                                      NULL|                46|               39|1051|       5|         Bogota|    514|514 - Plaza de la...|            LIMITADA|              31|   7104224|cra 69c   31  72 ...|                           NULL|            3|             17136501|ALARCON PINILLA J...|               0|             0|      NULL|     0.0|     NULL|   NULL|     NULL|                     N|        Solo Cliente|            N|      NULL|                   0|                   0|               NULL|            N|                 N|     CRA 91  19A  29|                N| INACTIVO|                 0| 1/04/2013|27/07/2016|  BAJO|Bodega|29/02/2024|        2|\n",
      "|     1|     3|900228986|SERVILOGISTICA IB...|     1| GUARIN ROJAS WILSON|  80354848|       0|DEFINIDA POR INST...|         30|          0| 3/06/2008|         73001000|         IBAGUE (TOLIMA)|                73|                COL|   2666714|            3/06/2008|                    73001000|             IBAGUE (TOLIMA)|                  73|                  COL|28/10/2008|         10|         10|          0|          0|          0|          0|       NULL|                                      NULL|              NULL|             NULL|4512|       1|           Cali|   1301|       1301 - Ibague|                OTRO|               0|   2666714|     cra.4b no.26-53|                           NULL|            0|                 NULL|                NULL|               0|             0|      NULL|     0.0|     NULL|   NULL|     NULL|                     N|        Solo Cliente|         NULL|      NULL|                NULL|                NULL|               NULL|         NULL|              NULL|CRA 4B N.26-53 HI...|             NULL| INACTIVO|                 0| 1/04/2013|30/12/2008|  BAJO|Bodega|31/01/2024|        2|\n",
      "|     1|     3|900680874|  ASESORES & BPO SAS|     1|JARAMILLO GALVAN ...|  32753361| 8001000|BARRANQUILLA (ATL...|          8|        COL|12/02/1991|          8001000|    BARRANQUILLA (ATL...|                 8|                COL|         1|           29/11/2013|                     8001000|        BARRANQUILLA (ATL...|                   8|                  COL|11/12/2013|        4,8|        3,4|        1,4|       10,3|       10,3|          1|          1|                                         1|                 1|                1|7020|       6|         Caribe|    801|801 - Barranquill...|Sociedad Anonima ...|              17|3148033161|        CRA 64 96 73|                        8001000|            1|             32753361|JARAMILLO GALVAN ...|               0|             0|      NULL|     0.0|     NULL|   NULL|     NULL|                     N|        Solo Cliente|            N|      NULL|                   0|                   0|               NULL|            N|                 N|    CLL 68 C  63  67|                N|   ACTIVO|                 0|23/06/2021|20/02/2023|  BAJO|Bodega|29/02/2024|        2|\n",
      "|     1|     3|900362050|  PANTHER DESIGN SAS|     1|MACIAS ACUna LAUR...|  52266645|11001000|   BOGOTA D.C. (D.C)|         11|        COL| 2/02/1995|         11001000|       BOGOTA D.C. (D.C)|                11|                COL|   3869103|            4/06/2010|                    11001000|           BOGOTA D.C. (D.C)|                  11|                  COL|13/06/2017|        700|        153|         50|        800|        720|         50|         12|                          NO OPERACIONALES|               831|              250|4330|       5|         Bogota|    510|      510 - Calle 80|Sociedad Anonima ...|               9|   6606574|CARRERA 69 R NUME...|                       11001000|            2|             52266645|MACIAS ACUNA LAUR...|               0|             0|      NULL|     0.0|     NULL|   NULL|     NULL|                     N|        Solo Cliente|            N|      NULL|                   0|                   0|               NULL|            N|                 N|CARRERA 68 B NUME...|                N| INACTIVO|                 0|13/06/2017|13/06/2017|  BAJO|Bodega|31/12/2023|        2|\n",
      "|     1|     3|901149031|M&A GESTION Y ASE...|     1|DAGUA VIVAS MARIELEN|  31792187|76834000|TULUA (VALLE DEL ...|         76|        COL|22/02/1999|         76834000|    TULUA (VALLE DEL ...|                76|                COL|3158420131|           23/01/2018|                    76834000|        TULUA (VALLE DEL ...|                  76|                  COL|27/06/2018|          1|          1|          2|          4|          2|          2|          1|                         VENTAS POR SERVIC|                 1|                1|7020|       1|           Cali|   1201|        1201 - Tulua|Sociedad Anonima ...|              31|   2333780|         CL 24 24 37|                           NULL|            2|             31792187|DAGUA VIVAS MARIELEN|               0|             0|      NULL|     0.0|     NULL|   NULL|     NULL|                     N|        Solo Cliente|            N|      NULL|                   0|                   0|               NULL|            N|                 N|         CL 24 24 37|                N| INACTIVO|                 0|27/06/2018|27/06/2018|  BAJO|Bodega|31/12/2023|        2|\n",
      "|     1|     3|900680894|SERVITRANSPODAS S...|     1|RESTREPO PAREJA G...|  31150982|76520000|PALMIRA (VALLE DE...|         76|        COL|24/03/1977|         76520000|    PALMIRA (VALLE DE...|                76|                COL|   2809424|            6/12/2013|                    76520000|        PALMIRA (VALLE DE...|                  76|                  COL|12/07/2016|        112|         11|        101|         12|         10|          1|          1|                                 GANANCIAS|                 2|                1| 161|       4|        Palmira|    401|     401 - Principal|Sociedad Anonima ...|              26|   2809424|       CL 37A  45-21|                       76520000|            1|             31150982|RESTREPO PAREJA G...|               0|             0|      NULL|     0.0|     NULL|   NULL|     NULL|                  NULL|        Solo Cliente|            N|      NULL|                   0|                   0|               NULL|            N|                 N|   CALLE 37A Ñ 45-21|             NULL| INACTIVO|                 0| 6/07/2023| 6/07/2023|  BAJO|Bodega|29/02/2024|        2|\n",
      "|     1|     3|900362094|BOTELLAS PLASTICA...|     1|ANEZ VILLEGAS PED...|  19195352|11001000|   BOGOTA D.C. (D.C)|         11|        COL|12/01/1972|         11001000|       BOGOTA D.C. (D.C)|                11|                COL|   4473111|            5/08/2014|                    11001000|           BOGOTA D.C. (D.C)|                  11|                  COL| 6/08/2014|       3243|       3971|        186|       3858|        406|        500|       NULL|                                      NULL|               322|               33|2599|       5|         Bogota|    560|560 - Banca Empre...|    SOCIEDAD ANONIMA|              26|   4473111|     CRA  68 B 12 66|                           NULL|            2|             19195352| PEDRO ANEZ VILLEGAS|               0|             0|      NULL|     0.0|     NULL|   NULL|     NULL|                  NULL|        Solo Cliente|            N|      NULL|                   0|                   0|               NULL|            N|                 N|      CRA 6 8B 12 66|                N| INACTIVO|                 0| 5/08/2014|06/03/2018|  BAJO|Bodega|31/12/2023|        2|\n",
      "|     1|     3|900681010|ATM DESARROLLO CI...|     1|TEJEIRO MEDINA AL...|  86048211|50001000|VILLAVICENCIO (META)|         50|        COL|13/12/1993|         50001000|    VILLAVICENCIO (META)|                50|                COL|3202839276|           19/09/2013|                    50001000|        VILLAVICENCIO (META)|                  50|                  COL| 9/01/2014|         30|          1|         29|         10|          1|          1|          1|                                        OT|                 1|                1|7111|       5|         Bogota|   2901|2901 - Villavicencio|Sociedad Anonima ...|               9|3202839276|CL 19 37 J 18 MZ ...|                           NULL|            1|             86048211|TEJEIRO MEDINA AL...|               0|             0|      NULL|     0.0|     NULL|   NULL|     NULL|                     N|        Solo Cliente|            N|      NULL|                   0|                   0|               NULL|            N|                 N|       CL 19 37 J 18|                N| INACTIVO|                 0|10/01/2014|10/01/2014|  BAJO|Bodega|29/02/2024|        2|\n",
      "|     1|     3|900681015|CONSORCIO JARDINE...|     1|ANGULO CORTES JOS...|  79418636|11001000|   BOGOTA D.C. (D.C)|         11|        COL|20/08/1985|         11001000|       BOGOTA D.C. (D.C)|                11|                COL|   3861790|           28/10/2013|                    11001000|           BOGOTA D.C. (D.C)|                  11|                  COL|13/12/2013|        800|          1|        800|       4100|       3200|        800|       NULL|                                      NULL|               341|             3200|4111|       5|         Bogota|    510|      510 - Calle 80|Union Temporal Co...|               9|   3861790|CRA 21 159 A 27 A...|                           NULL|            2|             79418636|ANGULO CORTES JOS...|               0|             0|      NULL|     0.0|     NULL|   NULL|     NULL|                     N|*No Cliente-No As...|         NULL|      NULL|                NULL|                NULL|               NULL|         NULL|              NULL|CRA 21 159 A 27 A...|                N| INACTIVO|                 0|13/12/2013|13/12/2013|  BAJO|Bodega|29/02/2024|        2|\n",
      "|     1|     9|193198654|    DISMEFAR DE JOAS|     1|ANDRADE VEGA LUIS...|         0|    NULL|                NULL|         30|          0| 1/01/2004|                0|    DEFINIDA POR INST...|                30|                  0|      NULL|            1/01/2004|                           0|        DEFINIDA POR INST...|                  30|                    0|21/04/2004|       NULL|       NULL|       NULL|       NULL|       NULL|       NULL|       NULL|                                      NULL|              NULL|             NULL|   0|       2|   Eje Cafetero|    601|601 - Armenia Pri...|SOCIEDAD PRIVADA ...|               0|      NULL|                NULL|                           NULL|            0|                 NULL|                NULL|               0|             0|      NULL|     0.0|     NULL|   NULL|     NULL|                     N|        Solo Cliente|         NULL|      NULL|                NULL|                NULL|               NULL|         NULL|              NULL|                NULL|             NULL| INACTIVO|                 0|      NULL|25/04/2023|  BAJO|Bodega|31/12/2023|        2|\n",
      "|     1|     3|900682163|SUMIRENT SALUD S....|     1|GARCES JURADO ANA...|  43871374| 5001000|MEDELLIN  (ANTIOQ...|          5|        COL| 5/11/1998|          5266000|    ENVIGADO (ANTIOQUIA)|                 5|                COL|   3410518|            9/12/2013|                     5266000|        ENVIGADO (ANTIOQUIA)|                   5|                  COL|31/01/2014|  310000000|    5000000|  310000000|   96000000|   42000000|   10000000|          1|                                         1|           8000000|          3500000|8621|       3|       Medellin|    304|  304 - Las Americas|Sociedad Anonima ...|              29|   3410518|Diag 75b No 2a 12...|                           NULL|            2|             43871374|GARCES JURADO ANA...|               0|             0|      NULL|     0.0|     NULL|   NULL|     NULL|                     N|        Solo Cliente|            N|      NULL|                   0|                   0|               NULL|            N|                 N|DIAG. 75 B NO 2A ...|                S|   ACTIVO|                 0| 4/02/2014| 4/02/2014|  BAJO|Bodega|29/02/2024|        2|\n",
      "|     1|     3|900682168|ALVARO CALDERON T...|     1|CALDERON TORO ALV...|  16625689|76834000|TULUA (VALLE DEL ...|         76|        COL|17/08/1977|         76001000|    CALI (VALLE DEL C...|                76|                COL|3148901168|            2/12/2013|                    76001000|        CALI (VALLE DEL C...|                  76|                  COL|12/12/2013|       6005|       4543|       1462|       1432|        668|          0|        133|                               FINANCIEROS|                19|               55|4390|       1|           Cali|   1201|        1201 - Tulua|Sociedad Anonima ...|               9|3148901168|        CL 116 23-06|                       76834000|            2|             16625689|CALDERON TORO ALV...|               0|             0|      NULL|     0.0|     NULL|   NULL|     NULL|                     N|        Solo Cliente|            N|      NULL|                   0|                   0|               NULL|            N|                 N|     CL 21  35 A- 07|                N|   ACTIVO|                 0|22/09/2023|26/01/2024|  BAJO|Bodega|29/02/2024|        2|\n",
      "|     1|     3|900682271|COLTRADING PETROL...|     1|BLANCO CORONEL MA...|1100892717|68001000|BUCARAMANGA (SANT...|         68|        COL|23/07/2009|         68615000|    RIONEGRO (SANTANDER)|                68|                COL|   6912882|           11/12/2013|                    68615000|        RIONEGRO (SANTANDER)|                  68|                  COL|14/09/2022|       4527|       3842|        685|       4676|       4616|         50|          2|                         BONO SOLIDARIO GO|               390|              385|4923|       3|       Medellin|   2101|  2101 - Bucaramanga|Sociedad Anonima ...|              16|   6912882|CR 34 51 80 OFIC ...|                       68001000|            1|             28333648|ANA JOAQUINA LIZA...|               0|             0|      NULL|     0.0|     NULL|   NULL|     NULL|                     N|        Solo Cliente|            N|      NULL|                   0|                   0|               NULL|            N|                 N|CR 34 51 80 OFIC ...|                N|   ACTIVO|                 0|14/09/2022|14/09/2022|  BAJO|Bodega|29/02/2024|        2|\n",
      "|     1|     3|900682356|DISTRIBUCIONES TE...|     1|SANCHEZ LEMOS JUA...|  16689248|76001000|CALI (VALLE DEL C...|         76|        COL|30/10/1981|         76001000|    CALI (VALLE DEL C...|                76|                COL|   3827550|           11/12/2013|                    76001000|        CALI (VALLE DEL C...|                  76|                  COL|25/09/2019|        215|        101|        114|        593|        246|         25|          1|                                        na|                49|                4|2819|       1|           Cali|    108|    108 - Chipichape|    SOCIEDAD ANONIMA|              31|   3827550|  cr 11d Ñ 33f  - 12|                       76001000|            1|             16689248|SANCHEZ LEMOS JUA...|               0|             0|      NULL|     0.0|        0|   NULL|        0|                     N|        Solo Cliente|            N|      NULL|                   0|                   0|               NULL|            N|                 N|  cr 11d Ñ 33f  - 12|                N| INACTIVO|                 0|13/09/2019|22/11/2022|  BAJO|Bodega|29/02/2024|        2|\n",
      "+------+------+---------+--------------------+------+--------------------+----------+--------+--------------------+-----------+-----------+----------+-----------------+------------------------+------------------+-------------------+----------+---------------------+----------------------------+----------------------------+--------------------+---------------------+----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------------------------------------+------------------+-----------------+----+--------+---------------+-------+--------------------+--------------------+----------------+----------+--------------------+-------------------------------+-------------+---------------------+--------------------+----------------+--------------+----------+--------+---------+-------+---------+----------------------+--------------------+-------------+----------+--------------------+--------------------+-------------------+-------------+------------------+--------------------+-----------------+---------+------------------+----------+----------+------+------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "El DataFrame tiene 57380 filas y 72 columnas.\n",
      "root\n",
      " |-- TIPCLI: integer (nullable = true)\n",
      " |-- TIPDOC: integer (nullable = true)\n",
      " |-- NIT: long (nullable = true)\n",
      " |-- NOMBRE: string (nullable = true)\n",
      " |-- TIPREP: integer (nullable = true)\n",
      " |-- REP_LEGAL: string (nullable = true)\n",
      " |-- NIT_REPRST: long (nullable = true)\n",
      " |-- CIU_REP: integer (nullable = true)\n",
      " |-- NOM_CIU_REP: string (nullable = true)\n",
      " |-- COD_DEP_REP: integer (nullable = true)\n",
      " |-- NOM_PAI_REP: string (nullable = true)\n",
      " |-- FECEXPREP: string (nullable = true)\n",
      " |-- CIUDAD_EXPEDICION: integer (nullable = true)\n",
      " |-- NOMBRE_CIUDAD_EXPEDICION: string (nullable = true)\n",
      " |-- NOM_DEP_EXPEDICIÓN: integer (nullable = true)\n",
      " |-- NOM_PAIS_EXPEDICIÓN: string (nullable = true)\n",
      " |-- TEL_REP: long (nullable = true)\n",
      " |-- FECHA_DE_CONSTITUCION: string (nullable = true)\n",
      " |-- CODIGO_LUGAR_DE_CONSTITUCION: integer (nullable = true)\n",
      " |-- NOMBRE_LUGAR_DE_CONSTITUCION: string (nullable = true)\n",
      " |-- COD_DEP_CONSTITUCIÓN: integer (nullable = true)\n",
      " |-- NOM_PAIS_CONSTITUCIÓN: string (nullable = true)\n",
      " |-- FEC_INGBCO: string (nullable = true)\n",
      " |-- TOT_ACTIVOB: string (nullable = true)\n",
      " |-- TOT_PASIVOB: string (nullable = true)\n",
      " |-- TOT_PTRMNOB: string (nullable = true)\n",
      " |-- TOT_VENTASB: string (nullable = true)\n",
      " |-- TOT_GASTOSB: string (nullable = true)\n",
      " |-- CAP_SOCIALB: string (nullable = true)\n",
      " |-- OTR_INGRSOB: string (nullable = true)\n",
      " |-- DETALLE_DE_OTROS_INGRESOS_NO_OPERACIONALES: string (nullable = true)\n",
      " |-- INGRESOS_MENSUALES: string (nullable = true)\n",
      " |-- EGRESOS_MENSUALES: string (nullable = true)\n",
      " |-- CIIU: integer (nullable = true)\n",
      " |-- REGIONAL: integer (nullable = true)\n",
      " |-- CODIGO_REGIONAL: string (nullable = true)\n",
      " |-- AGEHOMO: integer (nullable = true)\n",
      " |-- CODIGO_OFICNA_D: string (nullable = true)\n",
      " |-- TIPO_EMPRESA: string (nullable = true)\n",
      " |-- SECTOR_ECONOMICO: integer (nullable = true)\n",
      " |-- TEL_COMERC: string (nullable = true)\n",
      " |-- DIRECCION_COMERCIO: string (nullable = true)\n",
      " |-- CODIGO_CIUDAD_DE_SEDE_PRINCIPAL: integer (nullable = true)\n",
      " |-- NUMERO_SOCIOS: integer (nullable = true)\n",
      " |-- IDENTIFICACION_SOCIO1: string (nullable = true)\n",
      " |-- NOMBRE_SOCIO1: string (nullable = true)\n",
      " |-- TIPO_PRODUCTO_ME: integer (nullable = true)\n",
      " |-- ID_PRODUCTO_ME: string (nullable = true)\n",
      " |-- ENTIDAD_ME: string (nullable = true)\n",
      " |-- MONTO_ME: double (nullable = true)\n",
      " |-- CIUDAD_ME: string (nullable = true)\n",
      " |-- PAIS_ME: string (nullable = true)\n",
      " |-- MONEDA_ME: string (nullable = true)\n",
      " |-- REALIZA_OPERACIONES_ME: string (nullable = true)\n",
      " |-- TIP_VINCLC: string (nullable = true)\n",
      " |-- CARGO_PUBLICO: string (nullable = true)\n",
      " |-- CUAL_CARGO: string (nullable = true)\n",
      " |-- FECINI_CARGO_PUBLICO: string (nullable = true)\n",
      " |-- FECFIN_CARGO_PUBLICO: string (nullable = true)\n",
      " |-- DESEMP_CARGO_ACTUAL: string (nullable = true)\n",
      " |-- REP_LEGAL_ONG: string (nullable = true)\n",
      " |-- TIENE_VINCULO_PEPS: string (nullable = true)\n",
      " |-- Dir_Representante: string (nullable = true)\n",
      " |-- Posee_Cuentas_Ext: string (nullable = true)\n",
      " |-- ESTGESCOM: string (nullable = true)\n",
      " |-- VINCULACIONES_GURU: integer (nullable = true)\n",
      " |-- FECUAC_EF: string (nullable = true)\n",
      " |-- FECUAC: string (nullable = true)\n",
      " |-- SCORER: string (nullable = true)\n",
      " |-- Fuente: string (nullable = true)\n",
      " |-- FechaCorte: string (nullable = true)\n",
      " |-- id_activo: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Formatear el nombre de la tabla usando el nombre del activo\n",
    "nombre_tabla = f\"Calidad.{nombre_activo}\"\n",
    "\n",
    "# Llamar a la función para leer la tabla y obtener el DataFrame\n",
    "df_spark = leer_tabla_a_dataframe(url, nombre_tabla, usuario, contraseña, driver)\n",
    "\n",
    "# Mostrar el DataFrame resultante\n",
    "df_spark.show()\n",
    "\n",
    "# Obtener el número de filas\n",
    "num_filas = df_spark.count()\n",
    "\n",
    "# Obtener el número de columnas\n",
    "num_columnas = len(df_spark.columns)\n",
    "\n",
    "print(f\"El DataFrame tiene {num_filas} filas y {num_columnas} columnas.\")\n",
    "\n",
    "# Mostrar el esquema del DataFrame obtenido\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura Campo Activo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+-----------------+---------------------+\n",
      "|id_campo|nombre_campo|descripcion_campo|activo_informacion_id|\n",
      "+--------+------------+-----------------+---------------------+\n",
      "|     206|      SCORER|             NULL|                   12|\n",
      "|     207|     TEL_REP|             NULL|                   12|\n",
      "|     208|    REGIONAL|             NULL|                   12|\n",
      "+--------+------------+-----------------+---------------------+\n",
      "\n",
      "El DataFrame tiene 3 filas y 4 columnas.\n",
      "root\n",
      " |-- id_campo: long (nullable = true)\n",
      " |-- nombre_campo: string (nullable = true)\n",
      " |-- descripcion_campo: string (nullable = true)\n",
      " |-- activo_informacion_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, current_date, lit, to_date\n",
    "\n",
    "# Nombre de la tabla que se desea leer\n",
    "nombre_tabla = \"dbo.campo_activos\"\n",
    "\n",
    "# Llamar a la función para leer la tabla y obtener el DataFrame\n",
    "df_campoActivo = leer_tabla_a_dataframe(url, nombre_tabla, usuario, contraseña, driver)\n",
    "\n",
    "fecha_especifica = to_date(lit(\"2024-05-14\"), \"yyyy-MM-dd\")\n",
    "\n",
    "# Filtrar el DataFrame para obtener registros de hoy o fechas posteriores\n",
    "df_fil_campoActivo = df_campoActivo.filter(col(\"created_at\") >= fecha_especifica)\n",
    "\n",
    "df_fil_campoActivo = df_fil_campoActivo.drop(\"created_at\", \"updated_at\", \"tipo\", \"longitud\")\n",
    "\n",
    "df_fil_campoActivo = df_fil_campoActivo.withColumnRenamed(\"id\", \"id_campo\")\n",
    "df_fil_campoActivo = df_fil_campoActivo.withColumnRenamed(\"nombre\", \"nombre_campo\")\n",
    "df_fil_campoActivo = df_fil_campoActivo.withColumnRenamed(\"descripcion\", \"descripcion_campo\")\n",
    "\n",
    "# Mostrar los resultados filtrados\n",
    "df_fil_campoActivo.show()\n",
    "\n",
    "# Obtener el número de filas\n",
    "num_filas = df_fil_campoActivo.count()\n",
    "\n",
    "# Obtener el número de columnas\n",
    "num_columnas = len(df_fil_campoActivo.columns)\n",
    "\n",
    "print(f\"El DataFrame tiene {num_filas} filas y {num_columnas} columnas.\")\n",
    "\n",
    "# Mostrar el esquema del DataFrame obtenido\n",
    "df_fil_campoActivo.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura Regla "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+-----------+--------------------+----+-------------+\n",
      "|id_regla|        nombre_regla|   descripcion_regla|  dimension| descripcion_tecnica|tipo|tipo_regla_id|\n",
      "+--------+--------------------+--------------------+-----------+--------------------+----+-------------+\n",
      "|      45|   No puede ser cero|Esta regla valida...|    Validez|col(campo).isNull...|   0|         NULL|\n",
      "|      46|    No puede ser uno|Esta regla valida...|    Validez|col(campo).isNull...|   0|         NULL|\n",
      "|      47|No puede estar vacio|El campo no puede...|Completitud|col(campo).isNull...|   0|         NULL|\n",
      "+--------+--------------------+--------------------+-----------+--------------------+----+-------------+\n",
      "\n",
      "El DataFrame tiene 3 filas y 7 columnas.\n",
      "root\n",
      " |-- id_regla: long (nullable = true)\n",
      " |-- nombre_regla: string (nullable = true)\n",
      " |-- descripcion_regla: string (nullable = true)\n",
      " |-- dimension: string (nullable = true)\n",
      " |-- descripcion_tecnica: string (nullable = true)\n",
      " |-- tipo: integer (nullable = true)\n",
      " |-- tipo_regla_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Nombre de la tabla que se desea leer\n",
    "nombre_tabla = \"dbo.regla_filtros\"\n",
    "\n",
    "# Llamar a la función para leer la tabla y obtener el DataFrame\n",
    "df_reglaFiltros = leer_tabla_a_dataframe(url, nombre_tabla, usuario, contraseña, driver)\n",
    "\n",
    "\n",
    "df_fil_reglaFiltros= df_reglaFiltros.filter(col(\"created_at\") >= fecha_especifica)\n",
    "\n",
    "df_fil_reglaFiltros = df_fil_reglaFiltros.drop(\"created_at\", \"updated_at\", \"estado\")\n",
    "\n",
    "df_fil_reglaFiltros = df_fil_reglaFiltros.withColumnRenamed(\"id\", \"id_regla\")\n",
    "df_fil_reglaFiltros = df_fil_reglaFiltros.withColumnRenamed(\"nombre\", \"nombre_regla\")\n",
    "df_fil_reglaFiltros = df_fil_reglaFiltros.withColumnRenamed(\"descripcion\", \"descripcion_regla\")\n",
    "\n",
    "\n",
    "# Mostrar los resultados filtrados\n",
    "df_fil_reglaFiltros.show()\n",
    "\n",
    "# Obtener el número de filas\n",
    "num_filas = df_fil_reglaFiltros.count()\n",
    "\n",
    "# Obtener el número de columnas\n",
    "num_columnas = len(df_fil_reglaFiltros.columns)\n",
    "\n",
    "print(f\"El DataFrame tiene {num_filas} filas y {num_columnas} columnas.\")\n",
    "\n",
    "# Mostrar el esquema del DataFrame obtenido\n",
    "\n",
    "df_fil_reglaFiltros.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura Regla Campo Activo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+--------+\n",
      "|id_regla_campo|id_campo|id_regla|\n",
      "+--------------+--------+--------+\n",
      "|            54|     206|      45|\n",
      "|            55|     206|      46|\n",
      "|            56|     207|      46|\n",
      "|            57|     208|      47|\n",
      "+--------------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nombre_tabla = \"dbo.regla_campo_activos\"\n",
    "\n",
    "# Llamar a la función para leer la tabla y obtener el DataFrame\n",
    "df_regla_campo = leer_tabla_a_dataframe(url, nombre_tabla, usuario, contraseña, driver)\n",
    "\n",
    "\n",
    "df_regla_campo= df_regla_campo.filter(col(\"created_at\") >= fecha_especifica)\n",
    "\n",
    "df_regla_campo = df_regla_campo.drop(\"created_at\", \"updated_at\", \"regla_references\", \"activo_informacion_id\", \"nombre\")\n",
    "\n",
    "df_regla_campo = df_regla_campo.withColumnRenamed(\"id\", \"id_regla_campo\")\n",
    "df_regla_campo = df_regla_campo.withColumnRenamed(\"campo_activo_id\", \"id_campo\")\n",
    "df_regla_campo = df_regla_campo.withColumnRenamed(\"regla_filtro_id\", \"id_regla\")\n",
    "\n",
    "df_regla_campo.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generación de la dimensión REGLA_CAMPO_ACTIVO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+--------+--------------------+--------------------------------------------------+-----------+----------------------------------------------------------------------+----+-------------+------------+-----------------+---------------------+\n",
      "|id_regla_campo|id_campo|id_regla|nombre_regla        |descripcion_regla                                 |dimension  |descripcion_tecnica                                                   |tipo|tipo_regla_id|nombre_campo|descripcion_campo|activo_informacion_id|\n",
      "+--------------+--------+--------+--------------------+--------------------------------------------------+-----------+----------------------------------------------------------------------+----+-------------+------------+-----------------+---------------------+\n",
      "|57            |208     |47      |No puede estar vacio|El campo no puede presentar nulos o vacios        |Completitud|col(campo).isNull() |   (col(campo) == '')                            |0   |NULL         |REGIONAL    |NULL             |12                   |\n",
      "|56            |207     |46      |No puede ser uno    |Esta regla valida que el campo no sea uno o vacio |Validez    |col(campo).isNull() |   (col(campo) == '') |    (col(campo) == 1)     |0   |NULL         |TEL_REP     |NULL             |12                   |\n",
      "|55            |206     |46      |No puede ser uno    |Esta regla valida que el campo no sea uno o vacio |Validez    |col(campo).isNull() |   (col(campo) == '') |    (col(campo) == 1)     |0   |NULL         |SCORER      |NULL             |12                   |\n",
      "|54            |206     |45      |No puede ser cero   |Esta regla valida que el campo no sea cero o vacio|Validez    |col(campo).isNull() |   (col(campo) == '') |    (col(campo) == 0)     |0   |NULL         |SCORER      |NULL             |12                   |\n",
      "+--------------+--------+--------+--------------------+--------------------------------------------------+-----------+----------------------------------------------------------------------+----+-------------+------------+-----------------+---------------------+\n",
      "\n",
      "root\n",
      " |-- id_regla_campo: long (nullable = true)\n",
      " |-- id_campo: long (nullable = true)\n",
      " |-- id_regla: long (nullable = true)\n",
      " |-- nombre_regla: string (nullable = true)\n",
      " |-- descripcion_regla: string (nullable = true)\n",
      " |-- dimension: string (nullable = true)\n",
      " |-- descripcion_tecnica: string (nullable = true)\n",
      " |-- tipo: integer (nullable = true)\n",
      " |-- tipo_regla_id: long (nullable = true)\n",
      " |-- nombre_campo: string (nullable = true)\n",
      " |-- descripcion_campo: string (nullable = true)\n",
      " |-- activo_informacion_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Realizar el primer join entre df_regla_campo y df_fil_reglaFiltros\n",
    "df_joined = df_regla_campo.join(df_fil_reglaFiltros, \"id_regla\", \"inner\")\n",
    "\n",
    "# Realizar el segundo join con df_fil_campoActivo\n",
    "df_final = df_joined.join(df_fil_campoActivo, \"id_campo\", \"inner\")\n",
    "\n",
    "# Seleccionar las columnas deseadas para evitar duplicados\n",
    "df_final = df_final.select(\n",
    "    \"id_regla_campo\",\n",
    "    \"id_campo\",\n",
    "    \"id_regla\",\n",
    "    \"nombre_regla\",\n",
    "    \"descripcion_regla\",\n",
    "    \"dimension\",\n",
    "    \"descripcion_tecnica\",\n",
    "    \"tipo\",\n",
    "    \"tipo_regla_id\",\n",
    "    \"nombre_campo\",\n",
    "    \"descripcion_campo\",\n",
    "    \"activo_informacion_id\"\n",
    ")\n",
    "\n",
    "# Mostrar el DataFrame final para verificar\n",
    "df_final.show(truncate=False)\n",
    "df_final.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición de reglas para la calidad de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'REGIONAL': {'Completitud': ['No puede estar vacio']},\n",
      " 'SCORER': {'Validez': ['No puede ser uno', 'No puede ser cero']},\n",
      " 'TEL_REP': {'Validez': ['No puede ser uno']}}\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import expr, col, collect_list\n",
    "from pprint import pprint\n",
    "\n",
    "df_grouped = df_final.groupBy(\"activo_informacion_id\", \"nombre_campo\", \"dimension\").agg(\n",
    "    collect_list(\"nombre_regla\").alias(\"reglas\")\n",
    ")\n",
    "\n",
    "# Construir el diccionario de reglas_campos\n",
    "reglas_campos = {}\n",
    "for row in df_grouped.collect():\n",
    "    act_id = row[\"activo_informacion_id\"]\n",
    "    campo = row[\"nombre_campo\"]\n",
    "    dimension = row[\"dimension\"]\n",
    "    reglas = row[\"reglas\"]\n",
    "    \n",
    "    if campo not in reglas_campos:\n",
    "        reglas_campos[campo] = {}\n",
    "    if dimension not in reglas_campos[campo]:\n",
    "        reglas_campos[campo][dimension] = []\n",
    "    \n",
    "    reglas_campos[campo][dimension].extend(reglas)\n",
    "\n",
    "# Imprimir el diccionario con pprint\n",
    "pprint(reglas_campos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jerarquia de analisis QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Act ID: 12, Campo: TEL_REP, Dimension: Validez, Reglas Aplicadas: [((<function generate_expression.<locals>.expression_function at 0x7f9ad5312ca0>, 46, 207, 56), 'TEL_REP_Regla_No_puede_ser_uno')]\n",
      "Act ID: 12, Campo: SCORER, Dimension: Validez, Reglas Aplicadas: [((<function generate_expression.<locals>.expression_function at 0x7f9ad5312de0>, 46, 206, 55), 'SCORER_Regla_No_puede_ser_uno'), ((<function generate_expression.<locals>.expression_function at 0x7f9ad5312d40>, 45, 206, 54), 'SCORER_Regla_No_puede_ser_cero')]\n",
      "Act ID: 12, Campo: REGIONAL, Dimension: Completitud, Reglas Aplicadas: [((<function generate_expression.<locals>.expression_function at 0x7f9ad5311e40>, 47, 208, 57), 'REGIONAL_Regla_No_puede_estar_vacio')]\n",
      "{12: {'REGIONAL': {'Completitud': [((<function generate_expression.<locals>.expression_function at 0x7f9ad5311e40>,\n",
      "                                     47,\n",
      "                                     208,\n",
      "                                     57),\n",
      "                                    'REGIONAL_Regla_No_puede_estar_vacio')]},\n",
      "      'SCORER': {'Validez': [((<function generate_expression.<locals>.expression_function at 0x7f9ad5312de0>,\n",
      "                               46,\n",
      "                               206,\n",
      "                               55),\n",
      "                              'SCORER_Regla_No_puede_ser_uno'),\n",
      "                             ((<function generate_expression.<locals>.expression_function at 0x7f9ad5312d40>,\n",
      "                               45,\n",
      "                               206,\n",
      "                               54),\n",
      "                              'SCORER_Regla_No_puede_ser_cero')]},\n",
      "      'TEL_REP': {'Validez': [((<function generate_expression.<locals>.expression_function at 0x7f9ad5312ca0>,\n",
      "                                46,\n",
      "                                207,\n",
      "                                56),\n",
      "                               'TEL_REP_Regla_No_puede_ser_uno')]}}}\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import expr, col, collect_list\n",
    "from pprint import pprint\n",
    "\n",
    "# Asegúrate de que df_final tiene todas las columnas necesarias\n",
    "selected_df = df_final.select(\"nombre_campo\", \"nombre_regla\", \"descripcion_tecnica\", \"activo_informacion_id\", \"id_regla\", \"id_campo\", \"id_regla_campo\")\n",
    "\n",
    "def generate_expression(row):\n",
    "    regla_formatted = row.nombre_regla.replace(\" \", \"_\")\n",
    "    variable_name = f\"{row.nombre_campo}_Regla_{regla_formatted}\"\n",
    "    # Construir la expresión directamente como código Python evaluado\n",
    "    dynamic_expression = f\"col('{row.nombre_campo}').isNull() | (col('{row.nombre_campo}') == '') | (col('{row.nombre_campo}') == 1)\"\n",
    "    \n",
    "    # Función que evalúa la expresión dentro del contexto local adecuado\n",
    "    def expression_function(df):\n",
    "        # Local context para eval\n",
    "        local_dict = {'col': col, 'df': df, 'lit': F.lit}\n",
    "        return df.filter(eval(dynamic_expression, {}, local_dict))\n",
    "    \n",
    "    return (row.activo_informacion_id, variable_name, expression_function, row.id_regla, row.id_campo, row.id_regla_campo)\n",
    "\n",
    "# Aplica la función y recoge las expresiones\n",
    "expressions = selected_df.rdd.map(generate_expression).collect()\n",
    "expressions_dict = {}\n",
    "for act_id, name, func, id_regla, id_campo, id_regla_campo in expressions:\n",
    "    if act_id not in expressions_dict:\n",
    "        expressions_dict[act_id] = {}\n",
    "    expressions_dict[act_id][name] = (func, id_regla, id_campo, id_regla_campo)\n",
    "\n",
    "# Asegúrate de que df_grouped tiene la columna correcta\n",
    "df_grouped = df_final.groupBy(\"activo_informacion_id\", \"nombre_campo\", \"dimension\").agg(\n",
    "    collect_list(\"nombre_regla\").alias(\"reglas\")\n",
    ")\n",
    "\n",
    "reglas_campos = {}\n",
    "for row in df_grouped.collect():\n",
    "    act_id = row['activo_informacion_id']\n",
    "    campo = row['nombre_campo']\n",
    "    dimension = row['dimension']\n",
    "    reglas = row['reglas']\n",
    "    \n",
    "    if act_id not in reglas_campos:\n",
    "        reglas_campos[act_id] = {}\n",
    "    if campo not in reglas_campos[act_id]:\n",
    "        reglas_campos[act_id][campo] = {}\n",
    "    if dimension not in reglas_campos[act_id][campo]:\n",
    "        reglas_campos[act_id][campo][dimension] = []\n",
    "    \n",
    "    try:\n",
    "        reglas_aplicadas = [\n",
    "            (expressions_dict[act_id].get(f\"{campo}_Regla_{regla.replace(' ', '_')}\"), f\"{campo}_Regla_{regla.replace(' ', '_')}\") \n",
    "            for regla in reglas \n",
    "            if f\"{campo}_Regla_{regla.replace(' ', '_')}\" in expressions_dict[act_id]\n",
    "        ]\n",
    "        print(f\"Act ID: {act_id}, Campo: {campo}, Dimension: {dimension}, Reglas Aplicadas: {reglas_aplicadas}\")\n",
    "        reglas_campos[act_id][campo][dimension].extend(reglas_aplicadas)\n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar el campo '{campo}': {str(e)}\")\n",
    "\n",
    "pprint(reglas_campos)\n",
    "\n",
    "# Función para generar DataFrame desde un campo y sus reglas\n",
    "def generar_dataframe_desde_campo(campo, dataframe, reglas_por_campo):\n",
    "    df_campo = dataframe.select(campo, 'NIT', 'FechaCorte')\n",
    "    dataframes_concatenados = []\n",
    "\n",
    "    for dimension, filtros in reglas_por_campo.items():\n",
    "        for filtro_info, nombre_filtro in filtros:\n",
    "            filtro, id_regla, id_campo, id_regla_campo = filtro_info\n",
    "            df_filtrado = filtro(df_campo)\n",
    "            df_filtrado = df_filtrado.withColumn('Regla', F.lit(nombre_filtro))\n",
    "            df_filtrado = df_filtrado.withColumnRenamed(campo, 'Registro')\n",
    "            df_filtrado = df_filtrado.withColumn('Dimension', F.lit(dimension))\n",
    "            df_filtrado = df_filtrado.withColumn('Campo', F.lit(campo))\n",
    "            df_filtrado = df_filtrado.withColumn('id_regla', F.lit(id_regla))\n",
    "            df_filtrado = df_filtrado.withColumn('id_campo', F.lit(id_campo))\n",
    "            df_filtrado = df_filtrado.withColumn('id_regla_campo', F.lit(id_regla_campo))\n",
    "            df_filtrado = df_filtrado.withColumn('activo_informacion_id', F.lit(act_id))\n",
    "            dataframes_concatenados.append(df_filtrado)\n",
    "\n",
    "    if dataframes_concatenados:\n",
    "        df_concatenado = dataframes_concatenados[0]\n",
    "        for df in dataframes_concatenados[1:]:\n",
    "            df_concatenado = df_concatenado.union(df)\n",
    "        return df_concatenado\n",
    "    else:\n",
    "        return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejecución automatizada de calidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de inconformidades Fase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+----------+--------------------+---------+-------+--------+--------+--------------+---------------------+\n",
      "|Registro|      NIT|FechaCorte|               Regla|Dimension|  Campo|id_regla|id_campo|id_regla_campo|activo_informacion_id|\n",
      "+--------+---------+----------+--------------------+---------+-------+--------+--------+--------------+---------------------+\n",
      "|    NULL|  1619328|31/12/2023|TEL_REP_Regla_No_...|  Validez|TEL_REP|      46|     207|            56|                   12|\n",
      "|       1|900680874|29/02/2024|TEL_REP_Regla_No_...|  Validez|TEL_REP|      46|     207|            56|                   12|\n",
      "|    NULL|193198654|31/12/2023|TEL_REP_Regla_No_...|  Validez|TEL_REP|      46|     207|            56|                   12|\n",
      "|    NULL|900229681|31/01/2024|TEL_REP_Regla_No_...|  Validez|TEL_REP|      46|     207|            56|                   12|\n",
      "|    NULL|860044007|29/02/2024|TEL_REP_Regla_No_...|  Validez|TEL_REP|      46|     207|            56|                   12|\n",
      "|    NULL|860046238|29/02/2024|TEL_REP_Regla_No_...|  Validez|TEL_REP|      46|     207|            56|                   12|\n",
      "|       1|860049995|29/02/2024|TEL_REP_Regla_No_...|  Validez|TEL_REP|      46|     207|            56|                   12|\n",
      "|    NULL|860063952|29/02/2024|TEL_REP_Regla_No_...|  Validez|TEL_REP|      46|     207|            56|                   12|\n",
      "|    NULL|860091062|29/02/2024|TEL_REP_Regla_No_...|  Validez|TEL_REP|      46|     207|            56|                   12|\n",
      "|    NULL|860517408|29/02/2024|TEL_REP_Regla_No_...|  Validez|TEL_REP|      46|     207|            56|                   12|\n",
      "|    NULL|860519150|29/02/2024|TEL_REP_Regla_No_...|  Validez|TEL_REP|      46|     207|            56|                   12|\n",
      "|    NULL|860351849|29/02/2024|TEL_REP_Regla_No_...|  Validez|TEL_REP|      46|     207|            56|                   12|\n",
      "|    NULL|860403721|29/02/2024|TEL_REP_Regla_No_...|  Validez|TEL_REP|      46|     207|            56|                   12|\n",
      "|    NULL|860503634|29/02/2024|TEL_REP_Regla_No_...|  Validez|TEL_REP|      46|     207|            56|                   12|\n",
      "|    NULL|860503790|29/02/2024|TEL_REP_Regla_No_...|  Validez|TEL_REP|      46|     207|            56|                   12|\n",
      "|    NULL|860503990|29/02/2024|TEL_REP_Regla_No_...|  Validez|TEL_REP|      46|     207|            56|                   12|\n",
      "|    NULL|860526418|29/02/2024|TEL_REP_Regla_No_...|  Validez|TEL_REP|      46|     207|            56|                   12|\n",
      "|    NULL|860527421|29/02/2024|TEL_REP_Regla_No_...|  Validez|TEL_REP|      46|     207|            56|                   12|\n",
      "|       1|860529294|29/02/2024|TEL_REP_Regla_No_...|  Validez|TEL_REP|      46|     207|            56|                   12|\n",
      "|    NULL|860529713|29/02/2024|TEL_REP_Regla_No_...|  Validez|TEL_REP|      46|     207|            56|                   12|\n",
      "+--------+---------+----------+--------------------+---------+-------+--------+--------+--------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if activo_id is not None and activo_id in reglas_campos:\n",
    "    reglas_por_activo = reglas_campos[activo_id]\n",
    "    resultado_concatenado = None\n",
    "\n",
    "    for campo in reglas_por_activo:\n",
    "        try:\n",
    "            df_actual = generar_dataframe_desde_campo(campo, df_spark, reglas_por_activo[campo])\n",
    "            resultado_concatenado = df_actual if resultado_concatenado is None else resultado_concatenado.union(df_actual)\n",
    "        except Exception as e:\n",
    "            print(f\"Error al procesar el campo '{campo}': {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    if resultado_concatenado:\n",
    "        resultado_concatenado.show()\n",
    "else:\n",
    "    print(f\"No se encontraron reglas para el activo con ID {activo_id}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 26) (2286185397.py, line 26)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[42], line 26\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"\"\"\"\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 26)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# Agregar la columna \"ID_analisisQA\" con valor constante \"1\" al DataFrame \"resultado_concatenado\"\n",
    "RegistroNoConforme = resultado_concatenado.withColumn(\"ID_analisisQA\", lit(1))\n",
    "\n",
    "# Crear una ventana de partición para ordenar los registros y generar un ID único\n",
    "windowSpec = Window.orderBy(lit(1))\n",
    "RegistroNoConforme = RegistroNoConforme.withColumn(\"ID_Registro\", row_number().over(windowSpec))\n",
    "\n",
    "# Agregar la columna \"fecha_analisis\" con la fecha y hora actual\n",
    "RegistroNoConforme = RegistroNoConforme.withColumn(\"fecha_analisis\", current_timestamp())\n",
    "RegistroNoConforme = RegistroNoConforme.withColumn(\"fecha_remediacion\", lit(None).cast(\"timestamp\"))\n",
    "RegistroNoConforme = RegistroNoConforme.withColumn(\"estado\", lit(2))\n",
    "RegistroNoConforme = RegistroNoConforme.withColumn(\"plan_de_mejora_id\", lit(None).cast(\"long\"))\n",
    "RegistroNoConforme = RegistroNoConforme.withColumn(\"created_at\", current_timestamp())\n",
    "RegistroNoConforme = RegistroNoConforme.withColumn(\"updated_at\", current_timestamp())\n",
    "\n",
    "\n",
    "# Reordenar las columnas según lo solicitado\n",
    "df_reordenado = RegistroNoConforme.select('ID_Registro', 'id_regla', 'NIT', 'Registro', 'Dimension', 'ID_analisisQA', 'fecha_analisis', 'id_campo', 'activo_informacion_id', 'fecha_remediacion', 'estado', 'plan_de_mejora_id', 'created_at', 'updated_at')\n",
    "\n",
    "df_reordenado.show()\n",
    "\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+---------+------+---------+--------------+--------------------+---------------+---------------------+-----------------+------+-----------------+--------------------+--------------------+\n",
      "| id|regla_filtro_id|    llave|nombre|dimension|analisis_qa_id|      fecha_analisis|campo_activo_id|activo_informacion_id|fecha_remediacion|estado|plan_de_mejora_id|          created_at|          updated_at|\n",
      "+---+---------------+---------+------+---------+--------------+--------------------+---------------+---------------------+-----------------+------+-----------------+--------------------+--------------------+\n",
      "|  1|             46|  1619328|  NULL|  Validez|             1|2024-05-16 21:53:...|            207|                   12|             NULL|     2|             NULL|2024-05-16 21:53:...|2024-05-16 21:53:...|\n",
      "|  2|             46|900680874|     1|  Validez|             1|2024-05-16 21:53:...|            207|                   12|             NULL|     2|             NULL|2024-05-16 21:53:...|2024-05-16 21:53:...|\n",
      "|  3|             46|193198654|  NULL|  Validez|             1|2024-05-16 21:53:...|            207|                   12|             NULL|     2|             NULL|2024-05-16 21:53:...|2024-05-16 21:53:...|\n",
      "|  4|             46|900229681|  NULL|  Validez|             1|2024-05-16 21:53:...|            207|                   12|             NULL|     2|             NULL|2024-05-16 21:53:...|2024-05-16 21:53:...|\n",
      "|  5|             46|860044007|  NULL|  Validez|             1|2024-05-16 21:53:...|            207|                   12|             NULL|     2|             NULL|2024-05-16 21:53:...|2024-05-16 21:53:...|\n",
      "|  6|             46|860046238|  NULL|  Validez|             1|2024-05-16 21:53:...|            207|                   12|             NULL|     2|             NULL|2024-05-16 21:53:...|2024-05-16 21:53:...|\n",
      "|  7|             46|860049995|     1|  Validez|             1|2024-05-16 21:53:...|            207|                   12|             NULL|     2|             NULL|2024-05-16 21:53:...|2024-05-16 21:53:...|\n",
      "|  8|             46|860063952|  NULL|  Validez|             1|2024-05-16 21:53:...|            207|                   12|             NULL|     2|             NULL|2024-05-16 21:53:...|2024-05-16 21:53:...|\n",
      "|  9|             46|860091062|  NULL|  Validez|             1|2024-05-16 21:53:...|            207|                   12|             NULL|     2|             NULL|2024-05-16 21:53:...|2024-05-16 21:53:...|\n",
      "| 10|             46|860517408|  NULL|  Validez|             1|2024-05-16 21:53:...|            207|                   12|             NULL|     2|             NULL|2024-05-16 21:53:...|2024-05-16 21:53:...|\n",
      "| 11|             46|860519150|  NULL|  Validez|             1|2024-05-16 21:53:...|            207|                   12|             NULL|     2|             NULL|2024-05-16 21:53:...|2024-05-16 21:53:...|\n",
      "| 12|             46|860351849|  NULL|  Validez|             1|2024-05-16 21:53:...|            207|                   12|             NULL|     2|             NULL|2024-05-16 21:53:...|2024-05-16 21:53:...|\n",
      "| 13|             46|860403721|  NULL|  Validez|             1|2024-05-16 21:53:...|            207|                   12|             NULL|     2|             NULL|2024-05-16 21:53:...|2024-05-16 21:53:...|\n",
      "| 14|             46|860503634|  NULL|  Validez|             1|2024-05-16 21:53:...|            207|                   12|             NULL|     2|             NULL|2024-05-16 21:53:...|2024-05-16 21:53:...|\n",
      "| 15|             46|860503790|  NULL|  Validez|             1|2024-05-16 21:53:...|            207|                   12|             NULL|     2|             NULL|2024-05-16 21:53:...|2024-05-16 21:53:...|\n",
      "| 16|             46|860503990|  NULL|  Validez|             1|2024-05-16 21:53:...|            207|                   12|             NULL|     2|             NULL|2024-05-16 21:53:...|2024-05-16 21:53:...|\n",
      "| 17|             46|860526418|  NULL|  Validez|             1|2024-05-16 21:53:...|            207|                   12|             NULL|     2|             NULL|2024-05-16 21:53:...|2024-05-16 21:53:...|\n",
      "| 18|             46|860527421|  NULL|  Validez|             1|2024-05-16 21:53:...|            207|                   12|             NULL|     2|             NULL|2024-05-16 21:53:...|2024-05-16 21:53:...|\n",
      "| 19|             46|860529294|     1|  Validez|             1|2024-05-16 21:53:...|            207|                   12|             NULL|     2|             NULL|2024-05-16 21:53:...|2024-05-16 21:53:...|\n",
      "| 20|             46|860529713|  NULL|  Validez|             1|2024-05-16 21:53:...|            207|                   12|             NULL|     2|             NULL|2024-05-16 21:53:...|2024-05-16 21:53:...|\n",
      "+---+---------------+---------+------+---------+--------------+--------------------+---------------+---------------------+-----------------+------+-----------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "df_reg_inconforme= df_reordenado.withColumnRenamed(\"ID_Registro\", \"id\")\n",
    "df_reg_inconforme= df_reg_inconforme.withColumnRenamed(\"id_regla\", \"regla_filtro_id\")\n",
    "df_reg_inconforme= df_reg_inconforme.withColumnRenamed(\"NIT\", \"llave\")\n",
    "df_reg_inconforme = df_reg_inconforme.withColumnRenamed(\"Registro\", \"nombre\")\n",
    "df_reg_inconforme = df_reg_inconforme.withColumnRenamed(\"Dimension\", \"dimension\")\n",
    "df_reg_inconforme = df_reg_inconforme.withColumnRenamed(\"ID_analisisQA\", \"analisis_qa_id\")\n",
    "df_reg_inconforme = df_reg_inconforme.withColumnRenamed(\"fecha_analisis\", \"fecha_analisis\")\n",
    "df_reg_inconforme = df_reg_inconforme.withColumnRenamed(\"id_campo\", \"campo_activo_id\")\n",
    "df_reg_inconforme = df_reg_inconforme.withColumnRenamed(\"activo_informacion_id\", \"activo_informacion_id\")\n",
    "\n",
    "\n",
    "df_reg_inconforme.show()\n",
    "\"\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- regla_filtro_id: long (nullable = false)\n",
      " |-- llave: string (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- dimension: string (nullable = false)\n",
      " |-- analisis_qa_id: long (nullable = false)\n",
      " |-- fecha_analisis: timestamp (nullable = false)\n",
      " |-- campo_activo_id: long (nullable = false)\n",
      " |-- activo_informacion_id: long (nullable = false)\n",
      " |-- fecha_remediacion: timestamp (nullable = true)\n",
      " |-- estado: integer (nullable = false)\n",
      " |-- plan_de_mejora_id: long (nullable = true)\n",
      " |-- created_at: timestamp (nullable = false)\n",
      " |-- updated_at: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Convertir las columnas a los tipos de datos correspondientes en el segundo esquema\n",
    "df_reg_inconforme = df_reg_inconforme \\\n",
    "    .withColumn(\"id\", col(\"id\").cast(\"long\")) \\\n",
    "    .withColumn(\"regla_filtro_id\", col(\"regla_filtro_id\").cast(\"long\")) \\\n",
    "    .withColumn(\"llave\", col(\"llave\").cast(\"string\")) \\\n",
    "    .withColumn(\"analisis_qa_id\", col(\"analisis_qa_id\").cast(\"long\")) \\\n",
    "    .withColumn(\"campo_activo_id\", col(\"campo_activo_id\").cast(\"long\")) \\\n",
    "    .withColumn(\"activo_informacion_id\", col(\"activo_informacion_id\").cast(\"long\")) \\\n",
    "    .withColumn(\"estado\", col(\"estado\").cast(\"integer\")) \\\n",
    "    .withColumn(\"created_at\", col(\"created_at\").cast(\"timestamp\")) \\\n",
    "    .withColumn(\"updated_at\", col(\"updated_at\").cast(\"timestamp\"))\n",
    "\n",
    "# Asegurarse de que las columnas tienen el mismo orden\n",
    "df_reg_inconforme = df_reg_inconforme.select(\n",
    "    \"id\", \n",
    "    \"regla_filtro_id\", \n",
    "    \"llave\", \n",
    "    \"nombre\", \n",
    "    \"dimension\", \n",
    "    \"analisis_qa_id\", \n",
    "    \"fecha_analisis\", \n",
    "    \"campo_activo_id\", \n",
    "    \"activo_informacion_id\", \n",
    "    \"fecha_remediacion\", \n",
    "    \"estado\", \n",
    "    \"plan_de_mejora_id\", \n",
    "    \"created_at\", \n",
    "    \"updated_at\"\n",
    ")\n",
    "\n",
    "\n",
    "# Verificar el esquema\n",
    "df_reg_inconforme.printSchema()\n",
    "\"\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga de registros no conformes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o377.jdbc.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 66.0 failed 1 times, most recent failure: Lost task 0.0 in stage 66.0 (TID 52) (31f2cad8f1cd executor driver): java.sql.BatchUpdateException: Violation of PRIMARY KEY constraint 'PK_registro_no_conformes_id'. Cannot insert duplicate key in object 'dbo.registro_no_conformes'. The duplicate key value is (1).\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeBatch(SQLServerPreparedStatement.java:2231)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:746)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1036)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1034)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.sql.BatchUpdateException: Violation of PRIMARY KEY constraint 'PK_registro_no_conformes_id'. Cannot insert duplicate key in object 'dbo.registro_no_conformes'. The duplicate key value is (1).\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeBatch(SQLServerPreparedStatement.java:2231)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:746)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcargar_dataframe_a_tabla\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_reg_inconforme\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdbo.[registro_no_conformes]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m, in \u001b[0;36mcargar_dataframe_a_tabla\u001b[0;34m(dataframe, nombre_tabla)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mMétodo para cargar un DataFrame en una tabla de una base de datos.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m- nombre_tabla: Nombre de la tabla en la base de datos.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Escribe el DataFrame en la base de datos como una tabla\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mdataframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnombre_tabla\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43musuario\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpassword\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontraseña\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdriver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/sql/readwriter.py:1984\u001b[0m, in \u001b[0;36mDataFrameWriter.jdbc\u001b[0;34m(self, url, table, mode, properties)\u001b[0m\n\u001b[1;32m   1982\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m properties:\n\u001b[1;32m   1983\u001b[0m     jprop\u001b[38;5;241m.\u001b[39msetProperty(k, properties[k])\n\u001b[0;32m-> 1984\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjprop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o377.jdbc.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 66.0 failed 1 times, most recent failure: Lost task 0.0 in stage 66.0 (TID 52) (31f2cad8f1cd executor driver): java.sql.BatchUpdateException: Violation of PRIMARY KEY constraint 'PK_registro_no_conformes_id'. Cannot insert duplicate key in object 'dbo.registro_no_conformes'. The duplicate key value is (1).\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeBatch(SQLServerPreparedStatement.java:2231)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:746)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1036)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1034)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.sql.BatchUpdateException: Violation of PRIMARY KEY constraint 'PK_registro_no_conformes_id'. Cannot insert duplicate key in object 'dbo.registro_no_conformes'. The duplicate key value is (1).\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeBatch(SQLServerPreparedStatement.java:2231)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:746)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "#cargar_dataframe_a_tabla(df_reg_inconforme,\"dbo.[registro_no_conformes]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de inconformidades Fase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+----------+--------------------+---------+-------+--------+--------+--------------+---------------------+\n",
      "|Registro|      NIT|FechaCorte|               Regla|Dimension|  Campo|id_regla|id_campo|id_regla_campo|activo_informacion_id|\n",
      "+--------+---------+----------+--------------------+---------+-------+--------+--------+--------------+---------------------+\n",
      "|    NULL|  1619328|31/12/2023|TEL_REP_Regla_No_...|  Validez|TEL_REP|      46|     207|            56|                   12|\n",
      "|       1|900680874|29/02/2024|TEL_REP_Regla_No_...|  Validez|TEL_REP|      46|     207|            56|                   12|\n",
      "|    NULL|193198654|31/12/2023|TEL_REP_Regla_No_...|  Validez|TEL_REP|      46|     207|            56|                   12|\n",
      "|    NULL|900229681|31/01/2024|TEL_REP_Regla_No_...|  Validez|TEL_REP|      46|     207|            56|                   12|\n",
      "|    NULL|860044007|29/02/2024|TEL_REP_Regla_No_...|  Validez|TEL_REP|      46|     207|            56|                   12|\n",
      "|    NULL|860046238|29/02/2024|TEL_REP_Regla_No_...|  Validez|TEL_REP|      46|     207|            56|                   12|\n",
      "|       1|860049995|29/02/2024|TEL_REP_Regla_No_...|  Validez|TEL_REP|      46|     207|            56|                   12|\n",
      "|    NULL|860063952|29/02/2024|TEL_REP_Regla_No_...|  Validez|TEL_REP|      46|     207|            56|                   12|\n",
      "|    NULL|860091062|29/02/2024|TEL_REP_Regla_No_...|  Validez|TEL_REP|      46|     207|            56|                   12|\n",
      "|    NULL|860517408|29/02/2024|TEL_REP_Regla_No_...|  Validez|TEL_REP|      46|     207|            56|                   12|\n",
      "|    NULL|860519150|29/02/2024|TEL_REP_Regla_No_...|  Validez|TEL_REP|      46|     207|            56|                   12|\n",
      "|    NULL|860351849|29/02/2024|TEL_REP_Regla_No_...|  Validez|TEL_REP|      46|     207|            56|                   12|\n",
      "|    NULL|860403721|29/02/2024|TEL_REP_Regla_No_...|  Validez|TEL_REP|      46|     207|            56|                   12|\n",
      "|    NULL|860503634|29/02/2024|TEL_REP_Regla_No_...|  Validez|TEL_REP|      46|     207|            56|                   12|\n",
      "|    NULL|860503790|29/02/2024|TEL_REP_Regla_No_...|  Validez|TEL_REP|      46|     207|            56|                   12|\n",
      "|    NULL|860503990|29/02/2024|TEL_REP_Regla_No_...|  Validez|TEL_REP|      46|     207|            56|                   12|\n",
      "|    NULL|860526418|29/02/2024|TEL_REP_Regla_No_...|  Validez|TEL_REP|      46|     207|            56|                   12|\n",
      "|    NULL|860527421|29/02/2024|TEL_REP_Regla_No_...|  Validez|TEL_REP|      46|     207|            56|                   12|\n",
      "|       1|860529294|29/02/2024|TEL_REP_Regla_No_...|  Validez|TEL_REP|      46|     207|            56|                   12|\n",
      "|    NULL|860529713|29/02/2024|TEL_REP_Regla_No_...|  Validez|TEL_REP|      46|     207|            56|                   12|\n",
      "+--------+---------+----------+--------------------+---------+-------+--------+--------+--------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultado_concatenado.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar la columna \"ID_analisisQA\" con valor constante \"1\" al DataFrame \"resultado_concatenado\"\n",
    "reg_inconforme_tablero = resultado_concatenado.withColumn(\"ID_analisisQA\", lit(1))\n",
    "\n",
    "# Crear una ventana de partición para ordenar los registros y generar un ID único\n",
    "windowSpec = Window.orderBy(lit(1))\n",
    "reg_inconforme_tablero = reg_inconforme_tablero.withColumn(\"ID_Registro\", row_number().over(windowSpec))\n",
    "reg_inconforme_tablero = reg_inconforme_tablero.withColumn(\"NIT\", col(\"NIT\").cast(\"string\"))\n",
    "\n",
    "reg_inconforme_tablero= reg_inconforme_tablero.withColumnRenamed(\"id_regla\", \"id_regla_filtro\")\n",
    "reg_inconforme_tablero= reg_inconforme_tablero.withColumnRenamed(\"id_regla_campo\", \"id_regla\")\n",
    "reg_inconforme_tablero= reg_inconforme_tablero.withColumnRenamed(\"id_campo\", \"id_campoActivo\")\n",
    "reg_inconforme_tablero= reg_inconforme_tablero.withColumnRenamed(\"activo_informacion_id\", \"id_activo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Campo: string (nullable = false)\n",
      " |-- Regla: string (nullable = false)\n",
      " |-- ID_Registro: integer (nullable = false)\n",
      " |-- NIT: string (nullable = true)\n",
      " |-- Registro: string (nullable = true)\n",
      " |-- Dimension: string (nullable = false)\n",
      " |-- ID_analisisQA: integer (nullable = false)\n",
      " |-- FechaCorte: string (nullable = true)\n",
      " |-- id_regla: integer (nullable = false)\n",
      " |-- id_campoActivo: integer (nullable = false)\n",
      " |-- id_activo: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reg_inconforme_tablero=reg_inconforme_tablero.select('Campo','Regla','ID_Registro','NIT','Registro','Dimension','ID_analisisQA','FechaCorte','id_regla','id_campoActivo','id_activo')\n",
    "reg_inconforme_tablero.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cargar_dataframe_a_tabla(reg_inconforme_tablero, \"Calidad.RegCortTablero\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generacion etiquetado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+--------+--------------------+--------------------------------------------------+-----------+----------------------------------------------------------------------+----+-------------+------------+-----------------+---------------------+\n",
      "|id_regla_campo|id_campo|id_regla|nombre_regla        |descripcion_regla                                 |dimension  |descripcion_tecnica                                                   |tipo|tipo_regla_id|nombre_campo|descripcion_campo|activo_informacion_id|\n",
      "+--------------+--------+--------+--------------------+--------------------------------------------------+-----------+----------------------------------------------------------------------+----+-------------+------------+-----------------+---------------------+\n",
      "|57            |208     |47      |No puede estar vacio|El campo no puede presentar nulos o vacios        |Completitud|col(campo).isNull() |   (col(campo) == '')                            |0   |NULL         |REGIONAL    |NULL             |12                   |\n",
      "|56            |207     |46      |No puede ser uno    |Esta regla valida que el campo no sea uno o vacio |Validez    |col(campo).isNull() |   (col(campo) == '') |    (col(campo) == 1)     |0   |NULL         |TEL_REP     |NULL             |12                   |\n",
      "|55            |206     |46      |No puede ser uno    |Esta regla valida que el campo no sea uno o vacio |Validez    |col(campo).isNull() |   (col(campo) == '') |    (col(campo) == 1)     |0   |NULL         |SCORER      |NULL             |12                   |\n",
      "|54            |206     |45      |No puede ser cero   |Esta regla valida que el campo no sea cero o vacio|Validez    |col(campo).isNull() |   (col(campo) == '') |    (col(campo) == 0)     |0   |NULL         |SCORER      |NULL             |12                   |\n",
      "+--------------+--------+--------+--------------------+--------------------------------------------------+-----------+----------------------------------------------------------------------+----+-------------+------------+-----------------+---------------------+\n",
      "\n",
      "root\n",
      " |-- id_regla_campo: long (nullable = true)\n",
      " |-- id_campo: long (nullable = true)\n",
      " |-- id_regla: long (nullable = true)\n",
      " |-- nombre_regla: string (nullable = true)\n",
      " |-- descripcion_regla: string (nullable = true)\n",
      " |-- dimension: string (nullable = true)\n",
      " |-- descripcion_tecnica: string (nullable = true)\n",
      " |-- tipo: integer (nullable = true)\n",
      " |-- tipo_regla_id: long (nullable = true)\n",
      " |-- nombre_campo: string (nullable = true)\n",
      " |-- descripcion_campo: string (nullable = true)\n",
      " |-- activo_informacion_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.show(truncate=False)\n",
    "df_final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------------------+-----------+------------+---------+--------------------------------------------------+----------------------------------------------------------------------+\n",
      "|id_regla|Regla                              |dimension  |nombre Campo|id_activo|DescripcionRegla                                  |regla Tecnica                                                         |\n",
      "+--------+-----------------------------------+-----------+------------+---------+--------------------------------------------------+----------------------------------------------------------------------+\n",
      "|57      |REGIONAL_Regla_No_puede_estar_vacio|Completitud|REGIONAL    |12       |El campo no puede presentar nulos o vacios        |col(campo).isNull() |   (col(campo) == '')                            |\n",
      "|56      |TEL_REP_Regla_No_puede_ser_uno     |Validez    |TEL_REP     |12       |Esta regla valida que el campo no sea uno o vacio |col(campo).isNull() |   (col(campo) == '') |    (col(campo) == 1)     |\n",
      "|55      |SCORER_Regla_No_puede_ser_uno      |Validez    |SCORER      |12       |Esta regla valida que el campo no sea uno o vacio |col(campo).isNull() |   (col(campo) == '') |    (col(campo) == 1)     |\n",
      "|54      |SCORER_Regla_No_puede_ser_cero     |Validez    |SCORER      |12       |Esta regla valida que el campo no sea cero o vacio|col(campo).isNull() |   (col(campo) == '') |    (col(campo) == 0)     |\n",
      "+--------+-----------------------------------+-----------+------------+---------+--------------------------------------------------+----------------------------------------------------------------------+\n",
      "\n",
      "root\n",
      " |-- id_regla: long (nullable = true)\n",
      " |-- Regla: string (nullable = true)\n",
      " |-- dimension: string (nullable = true)\n",
      " |-- nombre Campo: string (nullable = true)\n",
      " |-- id_activo: long (nullable = true)\n",
      " |-- DescripcionRegla: string (nullable = true)\n",
      " |-- regla Tecnica: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "# Crear la nueva columna con el patrón especificado\n",
    "df_reg_tablero = df_final.withColumn(\n",
    "    \"Regla\",\n",
    "    F.concat(\n",
    "        F.col(\"nombre_campo\"),\n",
    "        F.lit(\"_Regla_\"),\n",
    "        F.regexp_replace(F.col(\"nombre_regla\"), \" \", \"_\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Renombrar las columnas\n",
    "df_reg_tablero = df_reg_tablero.withColumnRenamed(\"id_regla\", \"id_regla_filtro\")\n",
    "df_reg_tablero = df_reg_tablero.withColumnRenamed(\"id_regla_campo\", \"id_regla\")\n",
    "df_reg_tablero = df_reg_tablero.withColumnRenamed(\"nombre_campo\", \"nombre Campo\")\n",
    "df_reg_tablero = df_reg_tablero.withColumnRenamed(\"activo_informacion_id\", \"id_activo\")\n",
    "df_reg_tablero = df_reg_tablero.withColumnRenamed(\"descripcion_regla\", \"DescripcionRegla\")\n",
    "df_reg_tablero = df_reg_tablero.withColumnRenamed(\"descripcion_tecnica\", \"regla Tecnica\")\n",
    "\n",
    "# Seleccionar las columnas en el orden deseado\n",
    "df_reg_tablero = df_reg_tablero.select('id_regla', 'Regla', 'dimension', 'nombre Campo', 'id_activo', 'DescripcionRegla', 'regla Tecnica')\n",
    "\n",
    "# Mostrar el DataFrame resultante\n",
    "df_reg_tablero.show(truncate=False)\n",
    "df_reg_tablero.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cargar_dataframe_a_tabla(df_reg_tablero,\"Calidad.ReglaTablero\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode_pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
